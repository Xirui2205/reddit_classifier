#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import io
import re
import json
import time
import gzip
import math
import queue
import atexit
import random
import sqlite3
import logging
import hashlib
import argparse
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter, deque

# ---------------- CONFIG ----------------
INPUT_PATH = "kenya_clean_utf8.jsonl"   # .jsonl (default) or JSON array; .gz also supported
OUTPUT_DIR = Path("lang_split_out")
TEMP_PREFIX = OUTPUT_DIR / "temp_results_part"
CHECKPOINT_FILE = OUTPUT_DIR / "completed_ids.txt"
LOG_FILE = OUTPUT_DIR / "classifier.log"

MODEL = "deepseek-chat"
MAX_WORKERS = 12                 # conservative but fast
TARGET_BATCH_TOKENS = 2500       # sweet spot for stability/accuracy
BATCH_HARD_LIMIT = 15            # cap items in one call
RETRIES = 3
LONG_TEXT_LIMIT_TOKS = 600       # truncate ultra-long comments for stability
SLEEP_BETWEEN_BATCHES = 0.3
CHECKPOINT_FLUSH_N = 200         # buffer checkpoint writes
CACHE_DB = OUTPUT_DIR / "cache.sqlite3"

# ---------------- API KEY/BASE ----------------
import openai
openai.api_key = os.getenv("DEEPSEEK_API_KEY", "").strip()
openai.api_base = os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com").strip()

if not openai.api_key:
    raise SystemExit("Missing DEEPSEEK_API_KEY env var")

# ---------------- LOGGING ----------------
OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
logging.basicConfig(
    filename=str(LOG_FILE),
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
console = logging.StreamHandler()
console.setLevel(logging.INFO)
logging.getLogger("").addHandler(console)

# ---------------- TOKENIZER ----------------
try:
    import tiktoken
    _enc = tiktoken.get_encoding("cl100k_base")
    def estimate_tokens(txt: str) -> int:
        try:
            return len(_enc.encode(txt))
        except Exception:
            return max(1, int(len(txt) / 4))
except Exception:
    def estimate_tokens(txt: str) -> int:
        # fallback heuristic
        wc = len((txt or "").split())
        return max(wc, int(len(txt) * 0.32))

# ---------------- SKIP COUNTERS ----------------
skip_stats = Counter()

# ---------------- SQLITE PERSISTENT CACHE ----------------
def init_cache_db(path: Path):
    conn = sqlite3.connect(path)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS cache (
            h TEXT PRIMARY KEY,
            label TEXT NOT NULL
        )
    """)
    conn.commit()
    return conn

CACHE_CONN = init_cache_db(CACHE_DB)
CACHE_CUR = CACHE_CONN.cursor()
def cache_get(h: str):
    row = CACHE_CUR.execute("SELECT label FROM cache WHERE h=?", (h,)).fetchone()
    return row[0] if row else None

def cache_set_many(pairs):
    # pairs: list[(hash, label)]
    CACHE_CUR.executemany("INSERT OR REPLACE INTO cache(h,label) VALUES(?,?)", pairs)
    CACHE_CONN.commit()

@atexit.register
def _close_cache():
    try:
        CACHE_CONN.commit()
        CACHE_CONN.close()
    except Exception:
        pass

# ---------------- WRITER (buffered) ----------------
writer_q = queue.Queue()
part_index = 1
lines_in_part = 0
ckpt_buffer = []
ckpt_lock = False

def _flush_checkpoint_buffer():
    global ckpt_buffer
    if not ckpt_buffer:
        return
    with open(CHECKPOINT_FILE, "a", encoding="utf-8") as ckp:
        ckp.write("\n".join(ckpt_buffer) + "\n")
    ckpt_buffer = []

def writer_thread():
    global part_index, lines_in_part, ckpt_buffer
    current_path = f"{TEMP_PREFIX}_{part_index:03d}.jsonl"
    while True:
        recs = writer_q.get()
        if recs is None:
            break
        with open(current_path, "a", encoding="utf-8") as f:
            for r in recs:
                f.write(json.dumps(r, ensure_ascii=False) + "\n")
                lines_in_part += 1
                ckpt_buffer.append(r["id"])
                if len(ckpt_buffer) >= CHECKPOINT_FLUSH_N:
                    _flush_checkpoint_buffer()

        if lines_in_part >= 100_000:
            lines_in_part = 0
            part_index += 1
            current_path = f"{TEMP_PREFIX}_{part_index:03d}.jsonl"
    _flush_checkpoint_buffer()
    logging.info("Writer thread finished.")

import threading
threading.Thread(target=writer_thread, daemon=True).start()

# ---------------- UTILITIES ----------------
def safe_text_check(text: str) -> bool:
    t = (text or "").strip()
    return len(t) >= 2

def md5_hash(text: str) -> str:
    return hashlib.md5((text or "").encode("utf-8")).hexdigest()

def _open_any(path: str):
    return io.TextIOWrapper(gzip.open(path, "rb"), encoding="utf-8") if str(path).endswith(".gz") else open(path, "r", encoding="utf-8")

def validate_checkpoint():
    if not CHECKPOINT_FILE.exists():
        logging.info("No checkpoint file found — starting fresh.")
        return set()
    try:
        done = set(open(CHECKPOINT_FILE, "r", encoding="utf-8").read().split())
        total_lines = sum(1 for _ in open(INPUT_PATH, "r", encoding="utf-8")) if os.path.exists(INPUT_PATH) else 0
        pct = (len(done) / total_lines) * 100 if total_lines else 0
        logging.info(f"Resuming with {len(done):,}/{total_lines:,} ({pct:.1f}%) comments already processed.")
        return done
    except Exception as e:
        logging.warning(f"Checkpoint read failed ({e}) — starting fresh.")
        return set()

def get_unprocessed_lines(path: str, already_done_ids: set):
    if not os.path.exists(path):
        logging.error(f"Input file not found: {path}")
        return []

    # sniff
    with _open_any(path) as f:
        head = f.read(4096)
    is_array = head.lstrip().startswith("[")
    del head

    def _yield_jsonl():
        with _open_any(path) as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    rec = json.loads(line)
                except Exception:
                    continue
                cid = rec.get("id") or rec.get("_id") or rec.get("cid")
                if cid and cid in already_done_ids:
                    continue
                yield rec

    def _yield_array():
        with _open_any(path) as f:
            try:
                arr = json.load(f)
            except Exception as e:
                logging.error(f"Failed to parse JSON array: {e}")
                return
        for rec in arr:
            if not isinstance(rec, dict):
                continue
            cid = rec.get("id") or rec.get("_id") or rec.get("cid")
            if cid and cid in already_done_ids:
                continue
            yield rec

    return _yield_array() if is_array else _yield_jsonl()

# ---------------- HEURISTIC PRE-CLASSIFIER ----------------
# Fast pass to skip obvious cases (saves cost; keeps DeepSeek for ambiguous/Sheng)
SWAHILI_COMMON = set("""
na ya kwa ni sana bado bila kama ndio haya wewe sisi wapi leo jana kesho hivyo yake yetu wao wao
mimi wewe yeye huku pale ndani nje sasa pia kutu kitu vitu wakati wakati mwingine kwa sababu shukrani
""".split())

SHENG_HINTS = set("""
ndo msee buda wasee niko aje nikoarea nashida manze manzee dem mresh sherehe mtaa kejani nduthi
mbogi form ng'ara mafta gari jobo wacha bro brathe sheng
""".split())

def heuristic_label(text: str):
    t = (text or "").lower()
    if not t:
        return None
    # crude filters
    letters = sum(ch.isalpha() for ch in t)
    if letters < 3:
        return None

    # count swahili/common function words
    tokens = re.findall(r"[a-zA-Z']+", t)
    if not tokens:
        return None
    tok_set = set(tokens)

    sw_hits = sum(1 for w in tok_set if w in SWAHILI_COMMON)
    sh_hits  = sum(1 for w in tok_set if w in SHENG_HINTS)

    # strong English if very few non-ASCII words and no swahili/sheng hints
    if sw_hits == 0 and sh_hits == 0:
        # mostly ascii english words?
        if sum(1 for w in tokens if re.fullmatch(r"[a-zA-Z']+", w)) >= max(4, int(0.7 * len(tokens))):
            return "English"

    # strong Swahili signal
    if sw_hits >= 3 and sh_hits == 0:
        return "Swahili"

    # Sheng hint present
    if sh_hits >= 1:
        return "Sheng"

    # weak sw+en mix
    if 1 <= sw_hits <= 2:
        return "English and Swahili"

    return None

# ---------------- PROMPTS (JSON-LINES format) ----------------
SYSTEM_PROMPT = (
    "You are an expert linguist trained to classify Kenyan Reddit comments by language type.\n"
    "For EACH input line (a single JSON object with fields {id, text}), output ONE JSON line with exactly:\n"
    "{\"id\":\"...\",\"language\":\"<one of: Swahili | English and Swahili | Sheng | English>\"}\n"
    "CRITICAL:\n"
    "- Output JSON lines only, one per input line, same order, no markdown, no commentary.\n"
    "- Always pick exactly one label per id.\n"
    "- If uncertain between 'Sheng' and 'English and Swahili', prefer 'Sheng' when slangy/informal.\n"
)

def make_user_block(batch):
    # newline-separated JSON objects, each line an item
    # also truncate ultra-long by token count for stability
    lines = []
    for b in batch:
        txt = b["text"]
        if estimate_tokens(txt) > LONG_TEXT_LIMIT_TOKS:
            # conservative truncation at token/word boundary
            words = txt.split()
            words = words[: min(len(words), 2500)]
            txt = " ".join(words)
        lines.append(json.dumps({"id": b["id"], "text": txt}, ensure_ascii=False))
    return "\n".join(lines)

# ---------------- DEEPSEEK CALL ----------------
def deepseek_batch(batch):
    """
    Stable micro-batch caller with:
      - strict token budgeting
      - JSON-lines prompt format
      - content filter fallback (per-item)
      - retry & 429 handling
    Returns list[ {id, language} ] (no text field)
    """
    time.sleep(random.uniform(0.02, 0.15))

    content = make_user_block(batch)
    last_err = ""

    for attempt in range(1, RETRIES + 1):
        start = time.time()
        try:
            r = openai.ChatCompletion.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": content},
                ],
                temperature=0,
                max_tokens=1200,  # safe for <=15 outputs
            )
            txt = (r["choices"][0]["message"]["content"] or "").strip()
            # parse as JSON-LINES
            out = []
            for line in txt.splitlines():
                s = line.strip()
                if not s:
                    continue
                try:
                    obj = json.loads(s)
                except Exception:
                    # try bracket slice in case the model returned array chunk by mistake
                    if "[" in s and "]" in s:
                        try:
                            arr = json.loads(s[s.find("["): s.rfind("]") + 1])
                            for it in arr:
                                out.append({"id": it["id"], "language": it["language"]})
                            continue
                        except Exception:
                            pass
                    # skip malformed line
                    continue
                if "id" in obj and "language" in obj:
                    out.append({"id": obj["id"], "language": obj["language"]})
            # final sanity: ensure order & 1:1 mapping if possible
            got = {o["id"] for o in out if "id" in o}
            expect = [b["id"] for b in batch]
            if len(out) != len(batch) or any(eid not in got for eid in expect):
                # try soft alignment using first-come
                pass  # we accept partial for now; missing will be retried per-item below
            # attach meta for tuning (not used here but kept)
            latency = time.time() - start
            for o in out:
                o["_latency"] = latency
            return out

        except Exception as e:
            msg = str(e)
            last_err = msg

            # content filter path → fall back to per-item
            if "Content Exists Risk" in msg or "invalid_request_error" in msg:
                logging.warning("Content filter hit — falling back to per-item calls.")
                results = []
                for b in batch:
                    try:
                        single = json.dumps({"id": b["id"], "text": b["text"]}, ensure_ascii=False)
                        r2 = openai.ChatCompletion.create(
                            model=MODEL,
                            messages=[
                                {"role": "system", "content": SYSTEM_PROMPT},
                                {"role": "user", "content": single},
                            ],
                            temperature=0,
                            max_tokens=200,
                        )
                        line = (r2["choices"][0]["message"]["content"] or "").strip()
                        # expect single json line
                        obj = json.loads(line) if line.startswith("{") else json.loads(line[line.find("{"): line.rfind("}")+1])
                        if "id" in obj and "language" in obj:
                            results.append({"id": obj["id"], "language": obj["language"]})
                    except Exception as ie:
                        logging.warning(f"Per-item skip due to filter/parse: {ie}")
                        continue
                return results

            # rate limit
            if "429" in msg or "rate limit" in msg.lower():
                wait = 62 + random.uniform(0, 4)
                logging.warning(f"429 rate limit — sleeping {wait:.1f}s")
                time.sleep(wait)
                continue

            logging.warning(f"DeepSeek error attempt {attempt}/{RETRIES}: {msg}")
            time.sleep(1.5 * attempt)

    logging.warning(f"DeepSeek batch failed after retries. Last error: {last_err}")
    return []

# ---------------- BATCHER ----------------
class TokenBudgetBatcher:
    def __init__(self, target_tokens=TARGET_BATCH_TOKENS, hard_cap=BATCH_HARD_LIMIT):
        self.target_tokens = target_tokens
        self.hard_cap = hard_cap
        self.current = []
        self.current_tok = 0

    def add(self, item):
        t = estimate_tokens(item["text"])
        if (self.current and (self.current_tok + t > self.target_tokens or len(self.current) >= self.hard_cap)):
            full = self.current
            self.current = [item]
            self.current_tok = estimate_tokens(item["text"])
            return full
        else:
            self.current.append(item)
            self.current_tok += t
            return None

    def flush(self):
        if self.current:
            full = self.current
            self.current, self.current_tok = [], 0
            return full
        return None

# ---------------- MAIN ----------------
def split_to_final_files():
    counts = {"Swahili": 0, "English and Swahili": 0, "Sheng": 0, "English": 0, "Unknown": 0}
    outs = {
        "Swahili": open(OUTPUT_DIR / "swahili.jsonl", "w", encoding="utf-8"),
        "English and Swahili": open(OUTPUT_DIR / "mixed.jsonl", "w", encoding="utf-8"),
        "Sheng": open(OUTPUT_DIR / "sheng.jsonl", "w", encoding="utf-8"),
        "English": open(OUTPUT_DIR / "english.jsonl", "w", encoding="utf-8"),
    }

    try:
        for part in sorted(OUTPUT_DIR.glob("temp_results_part_*.jsonl")):
            with open(part, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        rec = json.loads(line)
                    except Exception:
                        continue
                    lang = rec.get("language") or rec.get("Language") or rec.get("label") or "Unknown"
                    if lang in outs:
                        outs[lang].write(line)
                        counts[lang] = counts.get(lang, 0) + 1
                    else:
                        counts["Unknown"] += 1
    finally:
        for fp in outs.values():
            fp.close()

    logging.info("📊 Final split summary:")
    for k, v in counts.items():
        logging.info(f"   {k:<22}: {v:,}")
    logging.info(f"📁 Split files in: {OUTPUT_DIR.absolute()}")

def main():
    start_time = time.time()
    done_ids = validate_checkpoint()
    stream = get_unprocessed_lines(INPUT_PATH, done_ids)
    if not stream:
        logging.info("Nothing to process — exiting.")
        return

    # count total for ETA
    try:
        total_lines = sum(1 for _ in open(INPUT_PATH, "r", encoding="utf-8"))
    except Exception:
        total_lines = 0

    logging.info("🚀 Starting classifier...")
    total = 0
    batcher = TokenBudgetBatcher()
    futures = []
    tok_hist = deque()
    cache_pairs = []  # for bulk sqlite insert

    def submit_batch(executor, batch):
        if not batch:
            return
        futures.append(executor.submit(deepseek_batch, batch))

    def write_results(results, original_by_id):
        nonlocal total, cache_pairs
        out = []
        for r in results:
            cid = r.get("id")
            lang = r.get("language")
            if not cid or not lang:
                continue
            text = original_by_id.get(cid, "")
            out.append({"id": cid, "text": text, "language": lang})

            # persistent cache
            cache_pairs.append((md5_hash(text), lang))

        if out:
            writer_q.put(out)
            total += len(out)

        if len(cache_pairs) >= 2000:
            cache_set_many(cache_pairs)
            cache_pairs = []

    # Maintain a sliding dict of originals for batch alignment
    original_map = {}

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        for rec in stream:
            cid = rec.get("id") or rec.get("_id") or rec.get("cid")
            text = rec.get("body") or rec.get("text") or ""
            if not cid or not safe_text_check(text):
                continue

            # de-dup by content via sqlite cache
            h = md5_hash(text)
            cached = cache_get(h)
            if cached:
                writer_q.put([{"id": cid, "text": text, "language": cached}])
                total += 1
                continue

            # fast heuristic (skip DeepSeek if confident)
            guess = heuristic_label(text)
            if guess:
                writer_q.put([{"id": cid, "text": text, "language": guess}])
                cache_pairs.append((h, guess))
                total += 1
                if len(cache_pairs) >= 2000:
                    cache_set_many(cache_pairs)
                    cache_pairs = []
                continue

            item = {"id": cid, "text": text}
            original_map[cid] = text
            ready = batcher.add(item)
            if ready:
                submit_batch(ex, ready)
                time.sleep(SLEEP_BETWEEN_BATCHES)

            # drain when futures pile up
            if len(futures) >= MAX_WORKERS * 2:
                sum_latency = 0.0
                meta_batches = 0
                for fut in as_completed(futures):
                    try:
                        result = fut.result()
                    except Exception as e:
                        logging.warning(f"Worker exception: {e}")
                        result = []
                    # map back to texts
                    if result:
                        write_results(result, original_map)
                        lat = result[0].get("_latency", 0.0)
                        sum_latency += float(lat)
                        meta_batches += 1
                futures.clear()

                elapsed = time.time() - start_time
                rate = total / elapsed if elapsed > 0 else 0.0
                avg_lat = (sum_latency / meta_batches) if meta_batches else 0.0
                logging.info(
                    f"✅ Progress: {total:,}/{total_lines:,} processed "
                    f"({rate:.1f}/s, avg_lat {avg_lat:.2f}s) | batch≈{BATCH_HARD_LIMIT}, workers={MAX_WORKERS}"
                )

        # tail batch
        tail = batcher.flush()
        if tail:
            futures.append(ex.submit(deepseek_batch, tail))

        # drain all
        for fut in as_completed(futures):
            try:
                result = fut.result()
            except Exception as e:
                logging.warning(f"Worker exception (tail): {e}")
                result = []
            if result:
                write_results(result, original_map)

    # final cache flush
    if cache_pairs:
        cache_set_many(cache_pairs)

    writer_q.put(None)
    elapsed = time.time() - start_time
    rate = total / elapsed if elapsed > 0 else 0.0
    logging.info(f"🏁 Done {total:,} new comments in {elapsed/3600:.2f}h ({rate:.1f}/s)")

    # split temps to final channel files
    split_to_final_files()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logging.warning("Interrupted by user.")
    except Exception as e:
        logging.exception(f"Fatal error: {e}")
